{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone Detection - ResNeXt R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.resnet import resnext101_32x8d\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "\n",
    "# Local nbutils.py\n",
    "import nbutils\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./data/drone-detection/drone-detection-new.v5-new-train.yolov8/train\"\n",
    "images_path = os.path.join(base_path, \"images\")\n",
    "labels_path = os.path.join(base_path, \"labels\")\n",
    "\n",
    "df = nbutils.create_dataset(images_path, labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary:\n",
      "Total number of objects: 8997\n",
      "Total number of unique images: 8818\n",
      "\n",
      "Class distribution:\n",
      "class\n",
      "DRONE         4349\n",
      "HELICOPTER    2374\n",
      "AIRPLANE      2274\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Image dimensions summary:\n",
      "       image_width  image_height\n",
      "count       8997.0        8997.0\n",
      "mean         640.0         640.0\n",
      "std            0.0           0.0\n",
      "min          640.0         640.0\n",
      "25%          640.0         640.0\n",
      "50%          640.0         640.0\n",
      "75%          640.0         640.0\n",
      "max          640.0         640.0\n"
     ]
    }
   ],
   "source": [
    "nbutils.view_df_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class mappings\n",
    "CLASS_MAPPING = {\"AIRPLANE\": 0, \"DRONE\": 1, \"HELICOPTER\": 2}\n",
    "\n",
    "# Custom Dataset\n",
    "class DroneDetectionDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "        self.image_paths = dataframe[\"image_path\"].unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get all annotations for this image\n",
    "        records = self.df[self.df[\"image_path\"] == image_path]\n",
    "\n",
    "        # Convert normalized YOLO box format to absolute pixel values\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        img_width, img_height = image.size\n",
    "\n",
    "        for _, row in records.iterrows():\n",
    "            x_center = row[\"x_center\"] * img_width\n",
    "            y_center = row[\"y_center\"] * img_height\n",
    "            width = row[\"width\"] * img_width\n",
    "            height = row[\"height\"] * img_height\n",
    "\n",
    "            # Convert YOLO format (x_center, y_center, width, height) to (x_min, y_min, x_max, y_max)\n",
    "            x_min = x_center - width / 2\n",
    "            y_min = y_center - height / 2\n",
    "            x_max = x_center + width / 2\n",
    "            y_max = y_center + height / 2\n",
    "\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(CLASS_MAPPING[row[\"class\"]])\n",
    "\n",
    "        # Convert to tensors\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Prepare target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DroneDetectionDataset(df, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load ResNeXt Backbone\n",
    "# backbone = resnext101_32x8d(pretrained=True)\n",
    "# backbone = torch.nn.Sequential(*list(backbone.children())[:-2])  # Remove final FC layer\n",
    "\n",
    "class CustomBackbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.body = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "        self.body = nn.Sequential(*list(self.body.children())[:-2])  # Remove FC layers\n",
    "        self.out_channels = 512  # ResNet18â€™s last conv layer has 512 output channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "# Instantiate custom backbone\n",
    "backbone = CustomBackbone()\n",
    "\n",
    "# Define the RPN (Region Proposal Network)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),),  # Multi-scale anchors\n",
    "    aspect_ratios=((0.5, 1.0, 2.0),) * 5\n",
    ")\n",
    "\n",
    "# Define Faster R-CNN model\n",
    "model = FasterRCNN(\n",
    "    backbone,\n",
    "    num_classes=4,  # Background + 3 object classes\n",
    "    rpn_anchor_generator=anchor_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TRAIN:\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and learning rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    num_epochs = 4\n",
    "\n",
    "    # Training function\n",
    "    def train_one_epoch(model, optimizer, dataloader, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        #for images, targets in dataloader:\n",
    "        for images, targets in tqdm(dataloader):\n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        loss = train_one_epoch(model, optimizer, dataloader, device)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(\"best_resnext.pth\", map_location=device))\n",
    "    model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Console Output:\n",
    "\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    #save the trained model\n",
    "    torch.save(model.state_dict(), \"best_resnext.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 0: Class 1, Score: 0.9396559000015259, Box: [109.87896728515625, 219.23574829101562, 137.22181701660156, 246.686767578125]\n",
      "Object 1: Class 1, Score: 0.7979368567466736, Box: [119.0453872680664, 548.5789184570312, 280.59716796875, 629.3258056640625]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, image_path, device):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# Test inference\n",
    "image_path = \"./data/drone-detection/drone-detection-new.v5-new-train.yolov8/test/images/V_DRONE_108218_224_png.rf.7f9bb4b54ced7c5578e6d584bd1108f6.jpg\"  # Replace with a real image path\n",
    "predictions = predict(model, image_path, device)\n",
    "\n",
    "# Display results\n",
    "for i, (box, label, score) in enumerate(zip(predictions[0][\"boxes\"], predictions[0][\"labels\"], predictions[0][\"scores\"])):\n",
    "    if score > 0.5:  # Filter out low-confidence detections\n",
    "        print(f\"Object {i}: Class {label.item()}, Score: {score.item()}, Box: {box.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_base_path = \"./data/drone-detection/drone-detection-new.v5-new-train.yolov8/test\"\n",
    "test_images_path = os.path.join(test_base_path, \"images\")\n",
    "test_labels_path = os.path.join(test_base_path, \"labels\")\n",
    "\n",
    "test_df = nbutils.create_dataset(test_images_path, test_labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = DroneDetectionDataset(test_df, transform=transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    metric = MeanAveragePrecision()  # Initialize mAP metric\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Move images to device\n",
    "            images = [img.to(device) for img in images]\n",
    "\n",
    "            # Move targets (boxes and labels) to device\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": t[\"boxes\"].to(device),\n",
    "                    \"labels\": t[\"labels\"].to(device),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "\n",
    "            # Model inference\n",
    "            predictions = model(images)\n",
    "\n",
    "            # Move predictions to CPU to avoid device mismatch\n",
    "            formatted_preds = [\n",
    "                {\n",
    "                    \"boxes\": pred[\"boxes\"].detach().cpu(),\n",
    "                    \"labels\": pred[\"labels\"].detach().cpu(),\n",
    "                    \"scores\": pred[\"scores\"].detach().cpu(),\n",
    "                }\n",
    "                for pred in predictions\n",
    "            ]\n",
    "\n",
    "            # Move targets to CPU to match the format of predictions\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": t[\"boxes\"].detach().cpu(),\n",
    "                    \"labels\": t[\"labels\"].detach().cpu(),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "\n",
    "            # Update metric\n",
    "            metric.update(formatted_preds, targets)\n",
    "\n",
    "    return metric.compute()  # Compute final mAP score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 119/119 [00:38<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'map': tensor(0.3353), 'map_50': tensor(0.6202), 'map_75': tensor(0.3182), 'map_small': tensor(0.2436), 'map_medium': tensor(0.3379), 'map_large': tensor(0.5100), 'mar_1': tensor(0.3765), 'mar_10': tensor(0.4074), 'mar_100': tensor(0.4074), 'mar_small': tensor(0.3136), 'mar_medium': tensor(0.4135), 'mar_large': tensor(0.5242), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([0, 1, 2], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "map_result = evaluate_model(model, test_dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "map: tensor(0.3353)\n",
      "map_50: tensor(0.6202)\n",
      "map_75: tensor(0.3182)\n",
      "map_small: tensor(0.2436)\n",
      "map_medium: tensor(0.3379)\n",
      "map_large: tensor(0.5100)\n",
      "mar_1: tensor(0.3765)\n",
      "mar_10: tensor(0.4074)\n",
      "mar_100: tensor(0.4074)\n",
      "mar_small: tensor(0.3136)\n",
      "mar_medium: tensor(0.4135)\n",
      "mar_large: tensor(0.5242)\n",
      "map_per_class: tensor(-1.)\n",
      "mar_100_per_class: tensor(-1.)\n",
      "classes: tensor([0, 1, 2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation Results:\")\n",
    "for k,v in map_result.items():\n",
    "    print(k +': ' + str(v))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
