{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone Detection - Mask R-CNN (ViT-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Local nbutils.py\n",
    "import nbutils\n",
    "from nbutils import DroneDetectionDataset\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"./data/drone-detection/drone-detection-new.v5-new-train.yolov8/train\"\n",
    "images_path = os.path.join(base_path, \"images\")\n",
    "labels_path = os.path.join(base_path, \"labels\")\n",
    "\n",
    "df = nbutils.create_dataset(images_path, labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary:\n",
      "Total number of objects: 8997\n",
      "Total number of unique images: 8818\n",
      "\n",
      "Class distribution:\n",
      "class\n",
      "DRONE         4349\n",
      "HELICOPTER    2374\n",
      "AIRPLANE      2274\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Image dimensions summary:\n",
      "       image_width  image_height\n",
      "count       8997.0        8997.0\n",
      "mean         640.0         640.0\n",
      "std            0.0           0.0\n",
      "min          640.0         640.0\n",
      "25%          640.0         640.0\n",
      "50%          640.0         640.0\n",
      "75%          640.0         640.0\n",
      "max          640.0         640.0\n"
     ]
    }
   ],
   "source": [
    "nbutils.view_df_summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter to convert original dataset output to Mask R-CNN format\n",
    "class DroneDetectionAdapter:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.classes = ['AIRPLANE', 'DRONE', 'HELICOPTER']\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images, boxes_batch = zip(*batch)\n",
    "        images_list = []\n",
    "        targets_list = []\n",
    "\n",
    "        for i, (image, boxes) in enumerate(zip(images, boxes_batch)):\n",
    "            # Process image\n",
    "            if not isinstance(image, torch.Tensor):\n",
    "                image = transforms.ToTensor()(image)\n",
    "            images_list.append(image.to(self.device))\n",
    "\n",
    "            # Get image dimensions\n",
    "            _, height, width = image.shape\n",
    "\n",
    "            # Process target\n",
    "            target = {}\n",
    "            valid_boxes = []\n",
    "            valid_labels = []\n",
    "            valid_masks = []\n",
    "\n",
    "            # Process each box\n",
    "            for box in boxes:\n",
    "                class_id = int(box[0].item())\n",
    "                \n",
    "                # Skip empty boxes (class_id = -1)\n",
    "                if class_id == -1:\n",
    "                    continue\n",
    "\n",
    "                # Extract normalized coordinates\n",
    "                x_center, y_center = float(box[1].item()), float(box[2].item())\n",
    "                box_width, box_height = float(box[3].item()), float(box[4].item())\n",
    "\n",
    "                # Convert to pixel coordinates and corner format\n",
    "                x1 = (x_center - box_width/2) * width\n",
    "                y1 = (y_center - box_height/2) * height\n",
    "                x2 = (x_center + box_width/2) * width\n",
    "                y2 = (y_center + box_height/2) * height\n",
    "\n",
    "                # Clip to image boundaries\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(width, x2), min(height, y2)\n",
    "\n",
    "                # Only add if box is valid\n",
    "                if x2 > x1 and y2 > y1:\n",
    "                    valid_boxes.append([x1, y1, x2, y2])\n",
    "                    valid_labels.append(class_id + 1)  # Add 1 because 0 is background in Mask R-CNN\n",
    "\n",
    "                    # Create a simple binary mask\n",
    "                    mask = torch.zeros((height, width), dtype=torch.uint8)\n",
    "                    x1_int, y1_int = int(x1), int(y1)\n",
    "                    x2_int, y2_int = int(x2), int(y2)\n",
    "                    mask[y1_int:y2_int, x1_int:x2_int] = 1\n",
    "                    valid_masks.append(mask)\n",
    "\n",
    "            # Handle empty case\n",
    "            if not valid_boxes:\n",
    "                target[\"boxes\"] = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                target[\"labels\"] = torch.zeros(0, dtype=torch.int64)\n",
    "                target[\"masks\"] = torch.zeros((0, height, width), dtype=torch.uint8)\n",
    "                target[\"image_id\"] = torch.tensor([i])\n",
    "                target[\"area\"] = torch.zeros(0, dtype=torch.float32)\n",
    "                target[\"iscrowd\"] = torch.zeros((0,), dtype=torch.int64)\n",
    "            else:\n",
    "                # Convert lists to tensors\n",
    "                boxes_tensor = torch.as_tensor(valid_boxes, dtype=torch.float32)\n",
    "                labels_tensor = torch.as_tensor(valid_labels, dtype=torch.int64)\n",
    "                masks_tensor = torch.stack(valid_masks)\n",
    "                \n",
    "                # Calculate areas\n",
    "                areas = (boxes_tensor[:, 2] - boxes_tensor[:, 0]) * (boxes_tensor[:, 3] - boxes_tensor[:, 1])\n",
    "                \n",
    "                # Create final target dictionary\n",
    "                target[\"boxes\"] = boxes_tensor\n",
    "                target[\"labels\"] = labels_tensor\n",
    "                target[\"masks\"] = masks_tensor\n",
    "                target[\"image_id\"] = torch.tensor([i])\n",
    "                target[\"area\"] = areas\n",
    "                target[\"iscrowd\"] = torch.zeros((len(valid_boxes),), dtype=torch.int64)\n",
    "\n",
    "            targets_list.append({k: v.to(self.device) for k, v in target.items()})\n",
    "\n",
    "        return images_list, targets_list\n",
    "\n",
    "def get_mask_rcnn_model(num_classes):\n",
    "    # Load pre-trained model\n",
    "    weights = MaskRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    model = maskrcnn_resnet50_fpn(weights=weights)\n",
    "    \n",
    "    # Get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the box predictor with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    # Get number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    # Replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, adapter):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, boxes in tqdm(data_loader):\n",
    "        # Convert dataset output to Mask R-CNN format\n",
    "        images, targets = adapter((images, boxes))\n",
    "        \n",
    "        # Forward pass and calculate loss\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += losses.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def bbox_iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Calculate IoU between box1 and box2\n",
    "    box1, box2: tensors of shape (4,) with coordinates [x1, y1, x2, y2]\n",
    "    Returns: scalar tensor with IoU value\n",
    "    \"\"\"\n",
    "    # Get coordinates of intersection\n",
    "    x1 = torch.max(box1[0], box2[0])\n",
    "    y1 = torch.max(box1[1], box2[1])\n",
    "    x2 = torch.min(box1[2], box2[2])\n",
    "    y2 = torch.min(box1[3], box2[3])\n",
    "    \n",
    "    # Calculate area of intersection and union\n",
    "    width = torch.clamp(x2 - x1, min=0)\n",
    "    height = torch.clamp(y2 - y1, min=0)\n",
    "    intersection = width * height\n",
    "    \n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    union = box1_area + box2_area - intersection\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "def evaluate(model, data_loader, device, adapter):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_ious = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, boxes in tqdm(data_loader):\n",
    "            # Convert dataset output to Mask R-CNN format\n",
    "            images, targets = adapter((images, boxes))\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            for i, (output, target) in enumerate(zip(outputs, targets)):\n",
    "                # Skip if no ground truth boxes\n",
    "                if len(target['boxes']) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Filter predictions by confidence\n",
    "                score_threshold = 0.5\n",
    "                keep_idxs = output['scores'] > score_threshold\n",
    "                pred_boxes = output['boxes'][keep_idxs]\n",
    "                pred_labels = output['labels'][keep_idxs]\n",
    "                pred_scores = output['scores'][keep_idxs]\n",
    "                \n",
    "                if len(pred_boxes) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # For each ground truth box, find best matching prediction\n",
    "                gt_boxes = target['boxes']\n",
    "                gt_labels = target['labels']\n",
    "                \n",
    "                for gt_idx, (gt_box, gt_label) in enumerate(zip(gt_boxes, gt_labels)):\n",
    "                    best_iou = 0\n",
    "                    best_pred_idx = -1\n",
    "                    \n",
    "                    # Find prediction with highest IoU\n",
    "                    for pred_idx, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
    "                        iou = bbox_iou(gt_box, pred_box)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_pred_idx = pred_idx\n",
    "                    \n",
    "                    # If we found a matching prediction\n",
    "                    if best_pred_idx >= 0:\n",
    "                        all_ious.append(best_iou.cpu().item())\n",
    "                        all_predictions.append(pred_labels[best_pred_idx].cpu().item())\n",
    "                        all_targets.append(gt_label.cpu().item())\n",
    "    \n",
    "    metrics = {}\n",
    "\n",
    "    if all_ious:\n",
    "        metrics['mean_iou'] = sum(all_ious) / len(all_ious)\n",
    "    \n",
    "    if all_predictions and all_targets:\n",
    "        metrics['f1'] = f1_score(all_targets, all_predictions, average='macro')\n",
    "        metrics['precision'] = precision_score(all_targets, all_predictions, average='macro', zero_division=0)\n",
    "        metrics['recall'] = recall_score(all_targets, all_predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_map(model, data_loader, device, adapter, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    ap_per_class = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Collect all predictions and ground truths\n",
    "        all_detections = {1: [], 2: [], 3: []}  # class_id -> list of [confidence, TP/FP]\n",
    "        num_gt_per_class = {1: 0, 2: 0, 3: 0}\n",
    "        \n",
    "        for images, boxes in tqdm(data_loader):\n",
    "            # Convert dataset output to Mask R-CNN format\n",
    "            images, targets = adapter((images, boxes))\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Process each image\n",
    "            for output, target in zip(outputs, targets):\n",
    "                # Count ground truths per class\n",
    "                for gt_label in target['labels']:\n",
    "                    class_id = gt_label.item()\n",
    "                    num_gt_per_class[class_id] = num_gt_per_class.get(class_id, 0) + 1\n",
    "                \n",
    "                # Skip empty predictions or targets\n",
    "                if len(output['boxes']) == 0 or len(target['boxes']) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # For each predicted box\n",
    "                for pred_idx, (pred_box, pred_score, pred_label) in enumerate(zip(output['boxes'], output['scores'], output['labels'])):\n",
    "                    class_id = pred_label.item()\n",
    "                    \n",
    "                    # Find best matching ground truth\n",
    "                    best_iou = 0\n",
    "                    best_gt_idx = -1\n",
    "                    \n",
    "                    for gt_idx, (gt_box, gt_label) in enumerate(zip(target['boxes'], target['labels'])):\n",
    "                        # Skip if not the same class\n",
    "                        if gt_label.item() != class_id:\n",
    "                            continue\n",
    "                            \n",
    "                        # Calculate IoU\n",
    "                        iou = bbox_iou(pred_box, gt_box)\n",
    "                        if iou > best_iou:\n",
    "                            best_iou = iou\n",
    "                            best_gt_idx = gt_idx\n",
    "                    \n",
    "                    # Determine if detection is TP or FP\n",
    "                    if best_gt_idx >= 0 and best_iou >= iou_threshold:\n",
    "                        all_detections[class_id].append([pred_score.item(), 1])  # TP\n",
    "                    else:\n",
    "                        all_detections[class_id].append([pred_score.item(), 0])  # FP\n",
    "        \n",
    "        # Calculate AP for each class\n",
    "        for class_id in all_detections:\n",
    "            if not all_detections[class_id] or num_gt_per_class.get(class_id, 0) == 0:\n",
    "                ap_per_class[class_id] = 0\n",
    "                continue\n",
    "                \n",
    "            # Sort by confidence\n",
    "            detections = sorted(all_detections[class_id], key=lambda x: x[0], reverse=True)\n",
    "            \n",
    "            # Calculate precision and recall\n",
    "            tp_cumsum = 0\n",
    "            fp_cumsum = 0\n",
    "            precision = []\n",
    "            recall = []\n",
    "            \n",
    "            for i, (_, is_tp) in enumerate(detections):\n",
    "                if is_tp:\n",
    "                    tp_cumsum += 1\n",
    "                else:\n",
    "                    fp_cumsum += 1\n",
    "                \n",
    "                precision.append(tp_cumsum / (tp_cumsum + fp_cumsum))\n",
    "                recall.append(tp_cumsum / num_gt_per_class[class_id])\n",
    "            \n",
    "            # Calculate AP using 11-point interpolation\n",
    "            ap = 0\n",
    "            for r in np.arange(0, 1.1, 0.1):\n",
    "                if not recall or recall[-1] < r:\n",
    "                    p = 0\n",
    "                else:\n",
    "                    p = max([precision[i] for i, rec in enumerate(recall) if rec >= r])\n",
    "                ap += p / 11\n",
    "            \n",
    "            ap_per_class[class_id] = ap\n",
    "    \n",
    "    # Calculate mAP\n",
    "    map_score = sum(ap_per_class.values()) / len(ap_per_class)\n",
    "    \n",
    "    return map_score, ap_per_class\n",
    "\n",
    "def train_mask_rcnn(train_loader, val_loader, num_epochs=20, learning_rate=0.001):\n",
    "    global device\n",
    "    train_adapter = DroneDetectionAdapter(device)\n",
    "    val_adapter = DroneDetectionAdapter(device)\n",
    "    \n",
    "    # 4 classes: background + 3 object classes (AIRPLANE, DRONE, HELICOPTER)\n",
    "    num_classes = 4\n",
    "    \n",
    "    # Get model\n",
    "    model = get_mask_rcnn_model(num_classes)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(params, lr=learning_rate, momentum=0.9, weight_decay=0.0005)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='max', \n",
    "        factor=0.1, \n",
    "        patience=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_f1 = 0.0\n",
    "    best_map = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, optimizer, train_loader, device, train_adapter)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate\n",
    "        metrics = evaluate(model, val_loader, device, val_adapter)\n",
    "        print(f\"Validation Metrics: {metrics}\")\n",
    "        \n",
    "        # Calculate mAP\n",
    "        map_score, ap_per_class = calculate_map(model, val_loader, device, val_adapter)\n",
    "        print(f\"mAP: {map_score:.4f}\")\n",
    "        print(f\"AP per class: {ap_per_class}\")\n",
    "        \n",
    "        # Update learning rate based on F1 score\n",
    "        if 'f1' in metrics:\n",
    "            lr_scheduler.step(metrics['f1'])\n",
    "            \n",
    "            # Save best model based on F1\n",
    "            if metrics['f1'] > best_f1:\n",
    "                best_f1 = metrics['f1']\n",
    "                torch.save(model.state_dict(), 'best_f1_model.pth')\n",
    "                print(f\"Saved new best F1 model with F1: {best_f1:.4f}\")\n",
    "        \n",
    "        # Save best model based on mAP\n",
    "        if map_score > best_map:\n",
    "            best_map = map_score\n",
    "            torch.save(model.state_dict(), 'best_map_model.pth')\n",
    "            print(f\"Saved new best mAP model with mAP: {best_map:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to make predictions with the trained model\n",
    "def predict(model, image_path, device, confidence_threshold=0.5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((800, 800)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    \n",
    "    # Extract predictions\n",
    "    pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "    pred_labels = predictions[0]['labels'].cpu().numpy()\n",
    "    pred_masks = predictions[0]['masks'].cpu().numpy()\n",
    "    \n",
    "    # Filter by confidence\n",
    "    high_conf_indices = pred_scores > confidence_threshold\n",
    "    pred_boxes = pred_boxes[high_conf_indices]\n",
    "    pred_scores = pred_scores[high_conf_indices]\n",
    "    pred_labels = pred_labels[high_conf_indices]\n",
    "    pred_masks = pred_masks[high_conf_indices]\n",
    "    \n",
    "    # Convert boxes from corner format back to center format\n",
    "    height, width = image.size\n",
    "    center_boxes = []\n",
    "    for box in pred_boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        x_center = (x1 + x2) / 2 / width\n",
    "        y_center = (y1 + y2) / 2 / height\n",
    "        box_width = (x2 - x1) / width\n",
    "        box_height = (y2 - y1) / height\n",
    "        center_boxes.append([x_center, y_center, box_width, box_height])\n",
    "    \n",
    "    # Create output\n",
    "    class_names = ['AIRPLANE', 'DRONE', 'HELICOPTER']\n",
    "    results = []\n",
    "    for i in range(len(pred_labels)):\n",
    "        class_id = pred_labels[i] - 1  # Convert back to 0-indexed\n",
    "        results.append({\n",
    "            'class': class_names[class_id] if 0 <= class_id < len(class_names) else 'UNKNOWN',\n",
    "            'confidence': float(pred_scores[i]),\n",
    "            'box': center_boxes[i]\n",
    "        })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ammarsiddiqui/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/882 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m train_loader = DataLoader(train_dataset, batch_size=\u001b[32m8\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m val_loader = DataLoader(val_dataset, batch_size=\u001b[32m8\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m model = \u001b[43mtrain_mask_rcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 344\u001b[39m, in \u001b[36mtrain_mask_rcnn\u001b[39m\u001b[34m(train_loader, val_loader, num_epochs, learning_rate)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    343\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_adapter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    347\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, device, adapter)\u001b[39m\n\u001b[32m    116\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, boxes \u001b[38;5;129;01min\u001b[39;00m tqdm(data_loader):\n\u001b[32m    119\u001b[39m     \u001b[38;5;66;03m# Convert dataset output to Mask R-CNN format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     images, targets = \u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# Forward pass and calculate loss\u001b[39;00m\n\u001b[32m    123\u001b[39m     loss_dict = model(images, targets)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mDroneDetectionAdapter.__call__\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     images, boxes_batch = \u001b[38;5;28mzip\u001b[39m(*batch)\n\u001b[32m      9\u001b[39m     images_list = []\n\u001b[32m     10\u001b[39m     targets_list = []\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "])\n",
    "\n",
    "dataset = DroneDetectionDataset(df, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = train_mask_rcnn(train_loader, val_loader, num_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
